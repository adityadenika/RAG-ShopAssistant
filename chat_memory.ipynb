{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e37bc239",
   "metadata": {},
   "source": [
    "# Chat Memory"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55429b33",
   "metadata": {},
   "source": [
    "### Import Library"
   ]
  },
  {
   "cell_type": "code",
   "id": "ad8f29d4",
   "metadata": {},
   "source": [
    "from haystack_experimental.chat_message_stores.in_memory import InMemoryChatMessageStore\n",
    "from haystack_experimental.components.retrievers import ChatMessageRetriever\n",
    "from haystack_experimental.components.writers import ChatMessageWriter\n",
    "from haystack.dataclasses import ChatMessage\n",
    "from haystack.components.joiners import ListJoiner\n",
    "from haystack import Pipeline\n",
    "from typing import List\n",
    "from haystack.components.builders import ChatPromptBuilder, PromptBuilder\n",
    "from haystack.components.generators.chat import OpenAIChatGenerator\n",
    "from haystack.components.generators import OpenAIGenerator\n",
    "from haystack.components.converters import OutputAdapter\n",
    "from haystack.utils import Secret\n",
    "from getpass import getpass\n",
    "import os"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "8a7c9319",
   "metadata": {},
   "source": [
    "os.environ[\"OPENAI_API_KEY\"] = getpass(\"Masukkan OpenAI API Key Anda: \")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "64e045d2",
   "metadata": {},
   "source": [
    "os.environ[\"MONGO_CONNECTION_STRING\"] = getpass(\"Masukkan MongoDB Connection String Anda: \")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "4a749d59",
   "metadata": {},
   "source": [
    "### Membuat Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e63dcaf7",
   "metadata": {},
   "source": [
    "membuat chat tanpa memory"
   ]
  },
  {
   "cell_type": "code",
   "id": "986beaf4",
   "metadata": {},
   "source": [
    "system_message = ChatMessage.from_system(\"You are a helpful assistant that answers questions based on the provided context.\")\n",
    "user_message_template = \"\"\"\n",
    "Answer the question based on the user query:\n",
    "query:{{query}}\n",
    "answer:\n",
    "\"\"\"\n",
    "user_message = ChatMessage.from_user(user_message_template)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "083ecec4",
   "metadata": {},
   "source": [
    "pipeline = Pipeline()\n",
    "pipeline.add_component(\"prompt_builder\", ChatPromptBuilder(variables=[\"query\"], required_variables=[\"query\"]))\n",
    "pipeline.add_component(\"generator\", OpenAIChatGenerator(model=\"gpt-4.1\", api_key=Secret.from_token(os.environ[\"OPENAI_API_KEY\"])))\n",
    "\n",
    "\n",
    "pipeline.connect(\"prompt_builder.prompt\", \"generator.messages\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "19a6e784",
   "metadata": {},
   "source": [
    "while True:\n",
    "    messages = [system_message, user_message]\n",
    "    query = input(\"Please input your question or type 'exit' to quit.\\n\")\n",
    "    if query.lower() == \"exit\":\n",
    "        break\n",
    "    res = pipeline.run(\n",
    "        {\n",
    "            \"prompt_builder\": {\n",
    "                \"query\": query,\n",
    "                \"template\":messages\n",
    "            }\n",
    "        }\n",
    "    )\n",
    "    print(\"AI Response:\", res[\"generator\"][\"replies\"][0].text)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "85ac151e",
   "metadata": {},
   "source": [
    "Membuat Pipeline dengan memory chat"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9805cf94",
   "metadata": {},
   "source": [
    "mendefinisikan InMemoryChatMessageStore terlebih dahulu"
   ]
  },
  {
   "cell_type": "code",
   "id": "60a27ecf",
   "metadata": {},
   "source": [
    "memory_store = InMemoryChatMessageStore()\n",
    "memory_retriever = ChatMessageRetriever(memory_store)\n",
    "memory_writer = ChatMessageWriter(memory_store)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "69f198ed",
   "metadata": {},
   "source": [
    "system_message = ChatMessage.from_system(\"You are a helpful assistant that answers questions based on the provided context.\")\n",
    "user_message_template = \"\"\"\n",
    "Answer the question based on the user query, please pay attention to the chat history:\n",
    "chat_history:\n",
    "{% for memory in memories %}\n",
    "    {{memory.text}}\n",
    "{% endfor %}\n",
    "\n",
    "query:{{query}}\n",
    "answer:\n",
    "\"\"\"\n",
    "user_message = ChatMessage.from_user(user_message_template)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "094593c9",
   "metadata": {},
   "source": [
    "pipeline = Pipeline()\n",
    "pipeline.add_component(\"prompt_builder\", ChatPromptBuilder(variables=[\"query\",\"memories\"], required_variables=[\"query\",\"memories\"]))\n",
    "pipeline.add_component(\"generator\", OpenAIChatGenerator(model=\"gpt-4.1\", api_key=Secret.from_token(os.environ[\"OPENAI_API_KEY\"])))\n",
    "pipeline.add_component(\"joiner\", ListJoiner(List[ChatMessage]))\n",
    "pipeline.add_component(\"memory_retriever\", memory_retriever)\n",
    "pipeline.add_component(\"memory_writer\", memory_writer)\n",
    "\n",
    "pipeline.connect(\"prompt_builder.prompt\", \"generator.messages\")\n",
    "pipeline.connect(\"generator.replies\", \"joiner\")\n",
    "pipeline.connect(\"joiner\", \"memory_writer\")\n",
    "pipeline.connect(\"memory_retriever\", \"prompt_builder.memories\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "91e07eff",
   "metadata": {},
   "source": [
    "while True:\n",
    "    messages = [system_message, user_message]\n",
    "    query = input(\"Please input your question or type 'exit' to quit.\\n\")\n",
    "    if query.lower() == \"exit\":\n",
    "        break\n",
    "    res = pipeline.run(\n",
    "        data={\n",
    "            \"prompt_builder\": {\n",
    "                \"query\": query,\n",
    "                \"template\":messages\n",
    "            },\n",
    "            \"joiner\":{\n",
    "                \"values\": [ChatMessage.from_user(query)]\n",
    "            }\n",
    "        },\n",
    "        include_outputs_from=[\"generator\"]\n",
    "    )\n",
    "    # print(res)\n",
    "    print(\"AI Response:\", res[\"generator\"][\"replies\"][0].text)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "76381e19",
   "metadata": {},
   "source": [],
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "shop_recommendation",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
